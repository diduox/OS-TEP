#### 基于锁的并发数据结构

在结束锁的讨论之前，我们先讨论如何在常见数据结构中使用锁。通过锁可以使数据结构线程安全（thread safe）。具体如何加锁决定了该数据结构的正确性和效率。

因此，我们的挑战是：

> **关键问题：如何给数据结构加锁？** 
>
> 对于特定数据结构，如何加锁才能让该结构功能正确？进一步，如何对该数据结构加锁，能够保证 高性能，让许多线程同时访问该结构，即并发访问（concurrently）？

##### 29.1 并发计数器

计数器是一种简单的数据结构，使用广泛而且接口简单。

```c
typedef struct counter_t {
	int value;
} counter_t;

void init(counter_t *c) {
	c->value = 0;
}

void increment(counter_t *c) {
	c->value++;
}

void decrement(counter_t *c) {
	c->value--;
}

int get(counter_t *c) {
	return c->value;
}
```

###### 简单但无法扩展

没有同步机制的计数器很简单，我们的下一个挑战是：如何让这段代码线程安全（thread safe）。

```c
typedef struct counter_t {
	int value;
	pthread_mutex_t lock;
} counter_t;

void init(counter_t *c) {
	c->value = 0;
	Pthread_mutex_init(&c->lock, NULL);
}

void increment(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	c->value++;
	Pthread_mutex_unlock(&c->lock);
}

void decrement(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	c->value--;
	Pthread_mutex_unlock(&c->lock);
}

int get(counter_t *c) {
	Pthread_mutex_lock(&c->lock);
	int rc = c->value;
	Pthread_mutex_unlock(&c->lock);
	return rc;
}
```

现在，有了一个并发数据结构，问题就是性能了。如果这个结构导致运行速度太慢，那么除了简单加锁，还需要进行优化。
请注意，如果数据结构导致的运行速度不是太慢，那就没事。如果简单的方案就能工作，就不需要精巧的设计。

为了理解简单方法的性能成本，我们运行一个基准测试，每个线程更新同一个共享计数器固定次数，然后我们改变线程数。图 29.3 给出了运行 1 个线程到 4 个线程的总耗时， 其中每个线程更新 100 万次计数器。本

<img src="D:\OS-TEP\OS-TEP\note\第29章 基于锁的并发数据结构\屏幕截图 2024-01-29 140822.png" style="zoom:80%;" />

上方的曲线（标为“精确”）可以看出，同步的计数器扩展性不好。单线程完成100万次更新执行只需要很短的的时间（大约0.03s），而两个线程并发执行，每个执行100万次，性能下降很多（超过5s！）。线程更多时，性能更差。

理想状态下，你会看到多处理（器）上运行的多线程就像单线程一样快。达到这种状态被称为完美扩展（perfect scaling）。虽然总工作量增多，但是并行执行完成后，完成任务的时间并没有增加。

###### 可扩展的计数

令人吃惊的是，关于如何实现可扩展的计数器。研究人员已经研究了很多年。

更令人吃惊的是，最近的操作系统性能分析研究表明，可扩展的计数器很重要。没有可扩展的计数，一些运行在Linux上的工作在多核机器上将遇到严重的扩展性问题。

我们通过懒惰计数器（sloppy counter）这一方法来解决这个问题。（这个方法是最近提出的）。

------

懒惰计数器通过多个局部计数器和一个全局计数器来实现一个逻辑计数器，其中每个CPU核心有一个局部计数器。具体来说，在 4 个 CPU 的机器上，有 4 个局部计数器和 1 个 全局计数器。除了这些计数器，还有锁：每个局部计数器有一个锁，全局计数器有一个。

懒惰计数器的基本思想：如果一个核心上的线程想要增加计数器，那么在局部锁的同步下增加自己的局部计数器。等待阈值S（sloppiness）到来时（即计数器的值>=S），局部计数器的值转移给全局计数器，并将自己清零。

S 越小，懒惰计数器则越趋近于非扩展的计数器。S 越大，扩展性越强，但是全局计数器与实际计数的偏差越大。

我们可以抢占所有的局部锁和全局锁（以特定的顺序，避免死锁），以获得精确值，但这种方法没有扩展性。

<img src="D:\OS-TEP\OS-TEP\note\第29章 基于锁的并发数据结构\屏幕截图 2024-01-29 143547.png" style="zoom:80%;" />

```c
typedef struct counter_t {
	int global; // global count
	pthread_mutex_t glock; // global lock
	int local[NUMCPUS]; // local count (per cpu)
	pthread_mutex_t llock[NUMCPUS]; // ... and locks
	int threshold; // update frequency
} counter_t;

// init: record threshold, init locks, init values
// of all local counts and global count
void init(counter_t *c, int threshold) {
	c->threshold = threshold;

	c->global = 0;
	pthread_mutex_init(&c->glock, NULL);

	int i;
	for (i = 0; i < NUMCPUS; i++) {
		c->local[i] = 0;
		pthread_mutex_init(&c->llock[i], NULL);
	}
}

// update: usually, just grab local lock and update local amount
// once local count has risen by 'threshold', grab global
// lock and transfer local values to it
void update(counter_t *c, int threadID, int amt) {
	pthread_mutex_lock(&c->llock[threadID]);//先拿局部锁
	c->local[threadID] += amt; // assumes amt > 0
	if (c->local[threadID] >= c->threshold) { // transfer to global
		pthread_mutex_lock(&c->glock);//如果大于S,再拿全局锁（此时既有局部锁，又有全局锁   ）
		c->global += c->local[threadID];
		pthread_mutex_unlock(&c->glock);
		c->local[threadID] = 0;
	}
	pthread_mutex_unlock(&c->llock[threadID]);
}

// get: just return global amount (which may not be perfect)
int get(counter_t *c) {
	pthread_mutex_lock(&c->glock);
	int val = c->global;
	pthread_mutex_unlock(&c->glock);
	return val; // only approximate!
}
```

##### 29.2 并发链表

简单起见，我们只关注链表的插入操作。

```c
// basic node structure
typedef struct node_t {
	int key;
	struct node_t *next;
} node_t;

// basic list structure (one used per list)
typedef struct list_t {
	node_t *head;
	pthread_mutex_t lock;
} list_t;

void List_Init(list_t *L) {
	L->head = NULL;
	pthread_mutex_init(&L->lock, NULL);
}

int List_Insert(list_t *L, int key) {
	pthread_mutex_lock(&L->lock);
	node_t *new = malloc(sizeof(node_t));
	if (new == NULL) {
		perror("malloc");//记录是malloc的错误
		pthread_mutex_unlock(&L->lock);//释放锁
		return -1; // fail
	}
	//头插法
	new->key = key;
	new->next = L->head;
	L->head = new;
	pthread_mutex_unlock(&L->lock);
	return 0; // success
}

int List_Lookup(list_t *L, int key) {
	pthread_mutex_lock(&L->lock);
	node_t *curr = L->head;
	while (curr) {
		if (curr->key == key) {//当访问到所需要的节点时
			pthread_mutex_unlock(&L->lock);
			return 0; // success
		}
		curr = curr->next;
	}
	pthread_mutex_unlock(&L->lock);
	return -1; // failure //当找不到所需要的节点时
}
```

从代码中可以看出，代码插入函数入口处获取锁，结束时释放锁。如果 malloc 失败（在 极少的时候），会有一点小问题，在这种情况下，代码在插入失败之前，必须释放锁。

事实表明，这种异常控制流容易产生错误。最近一个 Linux 内核补丁的研究表明，有 40%都是这种很少发生的代码路径（实际上，这个发现启发了我们自己的一些研究，我们从 Linux 文件系统中移除了所有内存失败的路径，得到了更健壮的系统[S+11]）

因此，挑战来了：我们能够重写插入和查找函数，保持并发插入正确，但避免在失败 情况下也需要调用释放锁吗？

具体来说，当我们假定malloc()是线程安全的，分配内存的时候其实并不需要锁。

我们只需要在真正的插入部分获取锁，释放锁即可。

PS：也就是说，让一个方法里出现两种释放锁的路径是不合适的，我们希望有一个单一的返回锁的路径。

```c 
void List_Init(list_t *L) {
	L->head = NULL;
	pthread_mutex_init(&L->lock, NULL);
}

void List_Insert(list_t *L, int key) {
	// synchronization not needed
	node_t *new = malloc(sizeof(node_t));
	if (new == NULL) {
		perror("malloc");
		return;
	}
	new->key = key;

	// just lock critical section
	pthread_mutex_lock(&L->lock);
	new->next = L->head;
	L->head = new;
	pthread_mutex_unlock(&L->lock);
}

int List_Lookup(list_t *L, int key) {
	int rv = -1;
	pthread_mutex_lock(&L->lock);
	node_t *curr = L->head;
	while (curr) {
		if (curr->key == key) {
			rv = 0;
			break;
		}
		curr = curr->next;
	}
	pthread_mutex_unlock(&L->lock);
	return rv; // now both success and failure 唯一返回路径
}
```

###### 扩展链表

尽管我们有了基本的并发链表，但又遇到了这个链表扩展性不好的问题。

研究人员发 现的增加链表并发的技术中，有一种叫作**过手锁**（hand-over-hand locking，也叫作**锁耦合**， lock coupling）[MS04]。

原理也很简单。每个节点都有一个锁，代替之前整个链表一个锁。遍历链表的时候，首先抢占笑一个节点的锁，然后释放当前节点的锁。

从概念上来说，过手锁链表有点道理（多个线程可以共同操作一个链表了），它增加了链表操作的并发程度。但是在实际上，在遍历的时候，每个节点获取锁，释放锁的开销巨大，很难比单锁的方法快。

即使有大量的线程和很大的链表，这种并发的方案也不一定会比单锁的方案快。也许某种杂合的方案（一定数量的节点用一个锁）值得去研究。

> **提示：更多并发不一定更快** 
>
> 如果方案带来了大量的开销（例如，频繁地获取锁、释放锁），那么高并发就没有什么意义。如果 简单的方案很少用到高开销的调用，通常会很有效。增加更多的锁和复杂性可能会适得其反。话虽如此， 有一种办法可以获得真知：实现两种方案（简单但少一点并发，复杂但多一点并发），测试它们的表现。 毕竟，你不能在性能上作弊。结果要么更快，要么不快

> **提示：当心锁和控制流** 
>
> 有一个通用建议，对并发代码和其他代码都有用，即注意控制流的变化导致函数返回和退出，或其 他错误情况导致函数停止执行。因为很多函数开始就会获得锁，分配内存，或者进行其他一些改变状态 的操作，如果错误发生，代码需要在返回前恢复各种状态，这容易出错。因此，最好组织好代码，减少 这种模式。

##### 29.3 并发队列 

综上所述，总有一个标准的方法来创建一个并发数据结构：添加一把大锁。对于这个队列，我们将跳过这种方法。

```c
typedef struct node_t {
	int value;
	struct node_t *next;
} node_t;

typedef struct queue_t {
	node_t *head;
	node_t *tail;
	pthread_mutex_t headLock;
	pthread_mutex_t tailLock;
} queue_t;

void Queue_Init(queue_t *q) {
	node_t *tmp = malloc(sizeof(node_t));
	tmp->next = NULL;
	q->head = q->tail = tmp;//这里创建的是头指针！！
	pthread_mutex_init(&q->headLock, NULL);
	pthread_mutex_init(&q->tailLock, NULL);
}

void Queue_Enqueue(queue_t *q, int value) {
	node_t *tmp = malloc(sizeof(node_t));

	assert(tmp != NULL);
	tmp->value = value;
	tmp->next = NULL;

	pthread_mutex_lock(&q->tailLock);
	q->tail->next = tmp;//插入新节点
	q->tail = tmp;//更新尾节点
	pthread_mutex_unlock(&q->tailLock);
}

int Queue_Dequeue(queue_t *q, int *value) {
	pthread_mutex_lock(&q->headLock);
	node_t *tmp = q->head;
	node_t *newHead = tmp->next;
	if (newHead == NULL) {//如果队列里面没节点了
		pthread_mutex_unlock(&q->headLock);
		return -1; // queue was empty
	}
	*value = newHead->value;//作为返回值（返回值是后一个，因为队列里第一个元素是头指针）
	q->head = newHead;//更新头指针
	pthread_mutex_unlock(&q->headLock);
	free(tmp);
	return 0;
}
```

##### 29.4 并发散列表

我们讨论最后一个应用广泛的并发数据结构，散列表（拉链寻址法散列表）。

```c
#define BUCKETS (101)

typedef struct hash_t {
	list_t lists[BUCKETS];//拉链
} hash_t;


void Hash_Init(hash_t *H) {
	int i;
	for (i = 0; i < BUCKETS; i++) {
		List_Init(&H->lists[i]);//初始化每个链表
	}
}

int Hash_Insert(hash_t *H, int key) {
	int bucket = key % BUCKETS;//哈希
	return List_Insert(&H->lists[bucket], key);//链表插入
}

int Hash_Lookup(hash_t *H, int key) {
	int bucket = key % BUCKETS;//哈希查找
	return List_Lookup(&H->lists[bucket], key);//遍历对应链表
}
```

本例的散列表使用我们之前实现的并发链 表，性能特别好。每个散列桶（每个桶都是一 个链表）都有一个锁，而不是整个散列表只有 一个锁，从而支持许多并发操作。

<img src="D:\OS-TEP\OS-TEP\note\第29章 基于锁的并发数据结构\屏幕截图 2024-01-29 155313.png" style="zoom:80%;" />

> 建议：避免不成熟的优化（Knuth 定律）
>
> 实现并发数据结构时，先从最简单的方案开始，也就是加一把大锁来同步。这样做，你很可能构建 了正确的锁。如果发现性能问题，那么就改进方法，只要优化到满足需要即可。正如 Knuth 的著名说法 “不成熟的优化是所有坏事的根源。”  
>
> 许多操作系统，在最初过渡到多处理器时都是用一把大锁，包括 Sun 和 Linux。在 Linux 中，这个 锁甚至有个名字，叫作 BKL（大内核锁，big kernel lock）。这个方案在很多年里都很有效，直到多 CPU 系统普及，内核只允许一个线程活动成为性能瓶颈。终于到了为这些系统优化并发性能的时候了。Linux 采用了简单的方案，把一个锁换成多个。Sun 则更为激进，实现了一个最开始就能并发的新系统，Solaris。 读者可以通过 Linux 和 Solaris 的内核资料了解更多信息。

##### 29.5 小结 

PS：第一次把小结写入笔记里面，因为很重要。

- **控制流变化时注意获取锁和释放锁；**
- **增加并发不一定能提高性能；** 
- **有性能问题的时候再做优化；**
- **避免不成熟的优化（premature optimization）， 对于所有关心性能的开发者都有用。我们让整个应用的某一小部分变快，却没有提高整体性能，其实没有价值**

当然，我们只触及了高性能数据结构的皮毛。（马里奥，公主还在另一个城堡ww）