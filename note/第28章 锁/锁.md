#### 锁

通过对并发的介绍，我们看到了并发编程的一个最基本问题：我们希望原子式执行一 系列指令，但由于单处理器上的中断（或者多个线程在多处理器上并发执行），我们做不到。 本章介绍了锁（lock），直接解决这一问题。程序员在源代码中加锁，放在临界区周围，保证临界区能够像单条原子指令一样执行。

##### 28.1 锁的基本思想

举个例子，假设临界区像这样，典型的更新共享变量：

```c
balance = balance + 1; 
```

​	当然，其他临界区也是可能的，比如为链表增加一个元素，或对共享结构的复杂更新 操作。为了使用锁，我们给临界区增加了这样一些代码：

```c
 lock_t mutex; // some globally-allocated lock 'mutex' 
 ... 
 lock(&mutex); 
 balance = balance + 1; 
 unlock(&mutex);
```

**锁就是一个变量**，我们必须声明类型，才能够使用。

锁的状态要么是可用的，表示没有线程持有锁；要么是被占用的，表示有一个线程持有锁，正处于临界区。锁也会包含其他信息，比如持有锁的线程，或者请求获取锁的线程队列，但这些信息会被隐藏起来，锁的使用者不会发现。

lock()和 unlock()函数的语义很简单。调用lock()表示获取锁，调用unlock()将锁变为可用。如果有等待线程（卡在lock()里），其中一个会（最终）注意到（或收到通知）锁状态的变化，获取该锁，进入临界区。

锁为程序员提供了最小程度的调度控制，将原本由操作系统调度的混乱状态变得更加可控。

##### 28.2 Pthread 锁 

POSIX库将锁称为互斥量（mutex），因为它被用来提供线程之间的互斥。

```c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; 
pthread_mutex_lock(&lock); // 其实可以封装一下这个函数，因为其可能调用不成功
balance = balance + 1; 
pthread_mutex_unlock(&lock);
```

POSIX的 lock 和 unlock 函数会传入一个变量，所以我们可以使用不同的锁来保护不同的变量。这样可以增加并发：不同于任何临界区都使用同一个大锁（粗粒度的锁策略），通常大家会用不同的锁保护不同的数据和结构，从而允许更多的线程进入临界区（细粒度的方案）。

##### 28.3 实现一个锁

> 关键问题：怎样实现一个锁 
>
> 如何构建一个高效的锁？高效的锁能够以低成本提供互斥，同时能够实现一些特性，我们下面会讨论。需要什么硬件支持？什么操作系统支持？

我们需要硬件和操作系统的帮助来实现一个可用的锁。近些年来，各种计算机体系结构的指令集都增加了一些不同的硬件原语，我们不研究这些指令是如何实现的，只研究如何使用他们来实现想锁这样的互斥原语。

##### 28.4 评价锁

为了评价锁是否能工作（并工作的好），我们应该先设立一些标准。

1. 有效性：锁是否能完成它的基本任务，即提供互斥。锁是否有效，能否阻止多个线程进入临界区。
2. 公平性：当锁可用时，是否每一个竞争线程有公平的机会抢到锁？是否有竞争锁的线程被饿死。一直无法获得锁？
3. 性能：即使用锁之后增加的时间开销。有几种情况需要考虑，一种是没有竞争的情况；另一种是一个CPU上多个线程竞争；最后一种是多个CPU、多个线程竞争时的性能。 

##### 28.5 控制中断 

最早提供的互斥解决方案之一，就是在临界区关闭中断。这个解决方案是为单处理器 系统开发的。代码如下：

```c
 void lock() { 
 	DisableInterrupts(); 
 } 
 void unlock() { 
 	EnableInterrupts(); 
 } 
```

 假设我们运行在一个单处理器系统上。通过在进入临界区之前关闭中断（使用特殊的硬件指令），可以保证临界区的代码不被中断，从而原子的执行。执行结束后，我们打开中断，程序正常运行。

优点：正确的实现了原子化，并且简单。

缺点：如果我们要在线程内关闭中断，这意为这我们允许所有线程执行特权操作，即信任这种机制不会被滥用。

众所周知，如果我们必须信任任意一个程序，我们可能就有麻烦了。

1. 一个贪婪的程序可能在其开始时就调用lock()，从而独占处理器。更糟的是，如果这个程序执行死循环，那系统无法重新获得控制，只能重启系统。

2. 这种方案不支持多处理器。如果多个线程运行在多个CPU上，不同CPU的线程可以试图进入同一个临界区，关闭中断也没有用。

3. 关闭中断会导致中断丢失，可能会导致严重的系统问题。即关闭中断的时候，可能有的设备产生中断却接收不到了。

   还有一个不重要的缺点就是效率太低，关闭和打开中断的操作CPU执行的较慢。

基于以上原因，只在很有限的情况下用关闭中断来实现互斥原语，例如，在某些情况下操作系统本身会采用屏蔽中断的方式，保证访问自己数据结构的原子性，或至少避免某些复杂的中断处理情况。这种做法时可行的，因为在操作系统内部不存在信任问题，它总是信任自己可以执行特权操作。

> ​																	**补充：DEKKER 算法和 PETERSON 算法**
> ​	20 世纪 60 年代，Dijkstra 向他的朋友们提出了并发问题，他的数学家朋友 Theodorus Jozef Dekker想出了一个解决方法。不同于我们讨论的需要硬件指令和操作系统支持的方法，Dekker 的算法（Dekker’s algorithm）只使用了 load 和 store（早期的硬件上，它们是原子的）。
> ​	Peterson 后来改进了 Dekker 的方法[P81]。同样只使用 load 和 store，保证不会有两个线程同时进入临界区。以下是 Peterson 算法（Peterson’s algorithm，针对两个线程），读者可以尝试理解这些代码吗？
>
> flag 和 turn 变量是用来做什么的？
>
> ```c
> int flag[2]; 
> int turn; 
> void init() { 
>      flag[0] = flag[1] = 0; // 1->thread wants to grab lock 
>      turn = 0; 				// whose turn? (thread 0 or 1?) 
> } 
> void lock() { 
>      flag[self] = 1; 		// self: thread ID of caller 
>      turn = 1 - self; 		// make it other thread's turn 
>      while ((flag[1-self] == 1) && (turn == 1 - self)) 
>      ; // spin-wait 
> } 
> void unlock() { 
>  	flag[self] = 0; 		// simply undo your intent 
> } 
> ```
>
> 一段时间以来，出于某种原因，大家都热衷于研究不依赖硬件支持的锁机制。后来这些工作都没有
> 太多意义，因为只需要很少的硬件支持，实现锁就会容易很多（实际在多处理器的早期，就有这些硬件支持）。而且上面提到的方法无法运行在现代硬件（应为松散内存一致性模型），导致它们更加没有用处。更多的相关研究也湮没在历史中…

简单分析一下代码

假设线程0想要控制锁

```c
						flag[0]    flag[1]    turn    self
init();						0		  0			0		0
lock();						1		  0         1       0
```

此时flag[1 - self] == flag[1] == 0,turn == 1 == 1-self 不进入循环

此时线程1也想控制锁

```c
						flag[0]    flag[1]    turn    self
							1		  0			1		1
lock();						1		  1         0       1
```

此时flag[1 - self] == flag[1] == 1,turn == 0 == 1-self 进入循环,直到线程0的锁被释放

------

假设线程0控制锁，并释放锁

```c
						flag[0]    flag[1]    turn    self
init();						0		  0			0		0
lock();						1		  0         1       0
unlock();					0		  0         1       0                         
```

所以说turn是一个标志量，其值并不影响某个线程是否能拿到锁。

flag[]表示是否有想获取锁的意图；turn表示应该轮到哪个线程进入临界区。

##### 28.6 测试并设置指令（原子交换） 

因为关闭中断的方法无法工作在多处理器上，所以系统设计者开始让硬件支持锁。最早的多处理器系统，已经有这些支持。今天所有的系统都支持，甚至包括单CPU的系统。

最简单的硬件支持是测试并设置指令(test -and-set instruction)，也叫做原子交换(atomic exchange)。为了理解test-and-set如何工作，我们首先实现一个不依赖它的锁，用一个变量标记锁是否持有。

```c
typedef struct lock_t {
	int flag;
} lock_t;

void init(lock_t *mutex) {
	// 0 -> lock is available, 1 -> held
	mutex->flag = 0;
}

void lock(lock_t *mutex) {
	while (mutex->flag == 1) // TEST the flag
		; // spin-wait (do nothing)
	mutex->flag = 1; // now SET it!
}

void unlock(lock_t *mutex) {
	mutex->flag = 0;
}
```

当第一个线程正处于临界区时，如果另一个线程调用lock()，它会在while循环中自旋等待（spin-wait），直到第一个线程调用unlock()清空标志。然后等待的线程会退出while循环。

但是这段代码有两个问题：正确性和性能。这个正确性问题在并发编程中很常见。

正确性问题：

在单处理器中，我们可以让中断发生在更改flag之前，使得两个线程都得到锁；

在多处理器中，两个线程可以同时进入lock()函数，导致两个线程都得到锁。

性能问题：

性能问题主要是线程在等待已经被持有的锁时，采用了自选等待(spin-waiting)的技术，就是不停的检查标志的值。自选等待在等待其他线程释放锁的时候会浪费时间（专注的等待，除了等待什么也不干）。尤其是在单处理器上，一个线程等待的目标线程甚至无法运行（至少在上下文切换前）。所以我们要开发出更成熟的解决方案。

##### 28.7 实现可用的自旋锁 

尽管上面例子的想法很好，但没有硬件的支持是无法实现的。幸运的是，一些系统提供了这一指令，支持基于这种概念创建简单的锁。这个更强大的指令有不同的名字：在 SPARC 上，这个指令叫 ldstub（load/store unsigned byte，加载/保存无符号字节）；在 x86 上，是 xchg （atomic exchange，原子交换）指令。但它们基本上在不同的平台上做同样的事，通常称为测试并设置指令（test-and-set）。我们用如下的 C 代码片段来定义测试并设置指令做了什么：

```c
int TestAndSet(int *old_ptr, int new) { 
 	int old = *old_ptr; 	// fetch old value at old_ptr 
 	*old_ptr = new; 		// store 'new' into old_ptr 
     return old; 			// return the old value 
 }
```

测试并设置指令做了下述事情。它返回 old_ptr 指向的旧值，同时更新为 new 的新值。并且这些指令都是原子地执行的。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫做“测试并设置”。

我们可以用它实现一个简单的自旋锁。

```c
typedef struct lock_t { 
 	int flag; 
 } lock_t; 
 
 void init(lock_t *lock) { 
 	// 0 indicates that lock is available, 1 that it is held 
 	lock->flag = 0; 
 } 
 
 void lock(lock_t *lock) { 
 	while (TestAndSet(&lock->flag, 1) == 1) 
 	; // spin-wait (do nothing) 
 } 
 
 void unlock(lock_t *lock) { 
 	lock->flag = 0; 
 } 
```

你现在可能也理解了为什么这种锁被称为自旋锁（spin lock）。这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（preemptive  scheduler，即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法 使用，因为一个自旋的线程永远不会放弃 CPU。

<u>*猜想：因为程序是状态机，所以每条代码的执行一定是有先后顺序的，所以不可能出现一条代码同时被两个线程执行导致锁失效。并发编程引入了多线程或多进程的情况，但在任何时刻，每个线程都是按照指令序列的顺序逐条执行的*</u>（写这里的时候有点神智不清了.......现在想想，如果出现这种状况，那么世界上所有的并发都不用写了）

##### 28.8 评价自旋锁 

现在可以按照之前的标准来评价基本的自旋锁了。锁最重要的一点是正确性：能够互斥吗？答案是可以的：自旋锁一次只允许一个线程进入临界区。因此，这是正确的锁。

下一个问题是公平性：自旋锁不提供任何公平性保障。实际上，自旋的线程在竞争条件下可能会永远自旋。自旋锁没有公平性，可能会导致饿死。

最后一个标准是性能：对于自旋锁，在单CPU的情况下，性能开销相当大。假设一个线程持有锁进入临界区 时被抢占。调度器可能会运行其他每一个线程（假设有 N−1 个这种线程）。而其他线程都在 竞争锁，都会在放弃 CPU 之前，自旋一个时间片，浪费 CPU 周期。

但是在多CPU上，自旋锁性能不错（如果线程数大致等于CPU数（线程太多就相当于单CPU了））。假设线程 A 在 CPU 1，线程 B 在 CPU 2 竞争同一个锁。线程 A（CPU 1）占有锁时，线程 B 竞争锁就会自 旋（在 CPU 2 上）。然而，临界区一般都很短，因此很快锁就可用，然后线程 B 获得锁。自 旋等待其他处理器上的锁，并没有浪费很多 CPU 周期，因此效果不错。（就是说自旋锁适用于短期独占的状况）

通俗来说，在多CPU自旋时，没有锁的线程在自旋，那个占有锁的线程在执行指令；而在单CPU自旋的情况下，无论是线程的自旋还是指令的执行，都在一个CPU上进行，也就是说，其他线程的竞争会影响执行的指令的那个线程，这又导致了其他线程更晚拿到锁（恶性循环了属于是）。

##### 28.9 比较并交换

某些系统提供了另一个硬件原语，即比较并交换指令（SPARC 系统中是 compare-and-swap， x86 系统是 compare-and-exchange）

```c
 int CompareAndSwap(int *ptr, int expected, int new) { 
     //将原值与ex比较，如果相等则让原值=新值（获取锁的操作）
     //不管怎样，函数都返回原值（判断是否自旋的操作）
     int actual = *ptr; 
     if (actual == expected) 
     *ptr = new; 
     return actual; 
 } 
```

lock()函数

```c
void lock(lock_t *lock) { 
    while (CompareAndSwap(&lock->flag, 0, 1) == 1) 
     ; // spin 
}
```

其余代码和上面测试并设置的例子完全一样。代码工作的方式很类似，检查标志是否 为 0，如果是，原子地交换为 1，从而获得锁。锁被持有时，竞争锁的线程会自旋。

最后，你可能会发现，比较并交换指令比测试并设置更强大。当我们在将来简单探讨 无等待同步（wait-free synchronization）时，会用到这条指令的强大之处。然而，如果 只用它实现一个简单的自旋锁，它的行为等价于上面分析的自旋锁

##### 28.10 链接的加载和条件式存储指令

一些平台提供了实现临界区的一对指令。例如 MIPS 架构[H93]中，链接的加载 （load-linked）和条件式存储（store-conditional）可以用来配合使用，实现其他并发结构。

```c
int LoadLinked(int *ptr) { //从内存中加载一个值，同时建立链接
	return *ptr; 
} 
int StoreConditional(int *ptr, int value) { //将一个值储存在内存中，只有链接的值才能存入，并且返回1 否则不存入，返回0
	if (no one has updated *ptr since the LoadLinked to this address) { 
		*ptr = value; 
		return 1; // success! 
	} else { 
		return 0; // failed to update 
	} 
} 
```

链接的加载指令和典型加载指令类似，都是从内存中取出值存入一个寄存器。

关键区 别来自条件式存储（store-conditional）指令，只有上一次加载的地址在期间都没有更新时， 才会成功，（同时更新刚才链接的加载的地址的值）。成功时，条件存储返回 1，并将 ptr 指 的值更新为 value。失败时，返回 0，并且不会更新值。

flag的初始值为0，代表锁没有被保持。

```c
void lock(lock_t *lock) {
	while (1) {
		while (LoadLinked(&lock->flag) == 1)//链接变量并判断锁是否被占用
			; // spin until it's zero
		if (StoreConditional(&lock->flag, 1) == 1)	//将flag的值设为1 如果flag之前是0，就return 			
			return; 	
	}
}

void unlock(lock_t *lock) {
	lock->flag = 0;
}
```

只有发生了中断才会导致条件式储存失败。

请注意条件式存储失败是如何发生的。一个线程调用 lock()，执行了链接的加载指令， 返回 0。在执行条件式存储之前，中断产生了，另一个线程进入 lock 的代码，也执行链接式 加载指令，同样返回 0。现在，两个线程都执行了链接式加载指令，将要执行条件存储。重 点是只有一个线程能够成功更新标志为 1，从而获得锁；第二个执行条件存储的线程会失败（因为另一个线程改变了链接值），必须重新尝试获取锁。

flag初始为0

```c
void lock(lock_t *lock) { 
//当链接的值为1 或者 当前当前flag的值和链接值相等时
// || 如果第一个为false，执行第二个
while (LoadLinked(&lock->flag)||!StoreConditional(&lock->flag, 1)) 
 ; // spin 
} 
```

##### 28.11 获取并增加

最后一个硬件原语是获取并增加（fetch-and-add）指令，它能原子地返回特定地址的旧 值，并且让该值自增一。

在这个例子中，我们会用获取并增加指令，实现一个更有趣的 ticket 锁（门票锁，旨在避免饥饿问题）。

```c
int FetchAndAdd(int *ptr) {
	int old = *ptr;
	*ptr = old + 1;
	return old;
}
typedef struct lock_t {
	int ticket;
	int turn;
} lock_t;

void lock_init(lock_t *lock) {
	lock->ticket = 0;
	lock->turn = 0;
}

void lock(lock_t *lock) {
	int myturn = FetchAndAdd(&lock->ticket);
	while (lock->turn != myturn)
		; // spin
}

void unlock(lock_t *lock) {
	FetchAndAdd(&lock->turn);
}
```

根据全局共享的 lock->turn 变量，当某一个线程的（myturn  == turn）时，则轮到这个线程进入临界区。unlock 则是增加 turn，从而下一个等待线程可以 进入临界区。

为什么其避免了饥饿问题呢？“

因为每个想要获取锁的线程的ticket值都不一样，即每个线程的myturn都不一样。

只有当myturn等于turn时，此线程才能获得锁。

就好像排队一样，每个想要获取锁的线程都按照先来后到叫了个号，之后等到其号码到达的时候，就能获取锁了。

##### 28.12 自旋过多：怎么办 

基于硬件的锁简单，而且有效，但是在某些场景下会导致效率低下。在单处理器上为例，当一个线程0持有锁时，其被中断。线程1去获取锁，然后其发现锁已经被持有。因此它就开始自旋，等待下一个之中中断的到来。

因此，类似的场景下，一个线程会 一直自旋检查一个不会改变的值，浪费掉整个时间片！

如果有 N 个线程去竞争一个锁，情 况会更糟糕。同样的场景下，会浪费 N−1 个时间片，只是自旋并等待一个线程释放该锁。

> 关键问题：怎样避免自旋
>
> 如何让锁不会不必要地自旋，浪费 CPU 时间？

只有硬件支持是不够的。我们还需要操作系统支持！接下来看一看怎么解决这一问题。

##### 28.13 简单方法：让出来吧，宝贝 

通过硬件，我们已经能实现有效、公平的锁。但是，问题依然存在：如果在临界区的线程发生了上下文切换，其他线程只能一直自旋，等待被中断的（持有锁的）线程重新运行。有什么好办法？

第一种简单友好的方法就是，在要自旋的时候，放弃 CPU。

```c
void init() { 
	flag = 0; 
} 

void lock() { 
	while (TestAndSet(&flag, 1) == 1) 
		yield(); // give up the CPU 
} 

void unlock() { 
	flag = 0; 
}
```

在这种方法中，我们假定操作系统提供原语yield(),线程可以调用它主动放弃CPU，让其他线程运行。

线程可以处于三种状态之一（运行、就绪和阻塞）。yield()系统调用能够让其运行态变为就绪=态，从而允许其他线程运行。因此，让出线程本质上取消调度了它自己。

在单CPU上运行两个进程。在这个例子中，基于yield的方法十分有效。一个线程调用lock(),发现锁被占用时，让出CPU，另外一个线程运行，完成临界区。

现在来考虑许多线程（例如 100 个）反复竞争一把锁的情况下。在这种情况下，一个线 程持有锁，在释放锁之前被抢占，其他 99 个线程分别调用 lock()，发现锁被抢占，然后让出 CPU。

假定采用某种轮转调度程序，这 99 个线程会一直处于运行—让出这种模式，直到 持有锁的线程再次运行。虽然比原来的浪费 99 个时间片的自旋方案要好，但这种方法仍然 成本很高，上下文切换的成本是实实在在的，因此浪费很大。

更糟的是，我们还没有考虑饿死的问题。一个线程可能一直处于让出的循环，而其他 线程反复进出临界区。很显然，我们需要一种方法来解决这个问题。

**28.14 使用队列：休眠替代自旋**

前面一些方法的真正问题是存在太多的偶然性。调度程序决定如何调度。如果调度不 合理，线程或者一直自旋（第一种方法），或者立刻让出 CPU（第二种方法）。无论哪种方 法，都可能造成浪费，也能防止饿死。

因此，我们必须显式地施加某种控制，决定锁释放时，谁能抢到锁。为了做到这一点， 我们需要操作系统的更多支持，并需要一个队列来保存等待锁的线程。

简单起见，我们利用 Solaris 提供的支持，它提供了两个调用：park()能够让调用线程休 眠，unpark(threadID)则会唤醒 threadID 标识的线程。可以用这两个调用来实现锁，让调用 者在获取不到锁时睡眠，在锁可用时被唤醒。

```c
typedef struct lock_t {
	int flag;
	int guard;
	queue_t *q;//线程队列，存储等待锁的线程
} lock_t;

void lock_init(lock_t *m) {
	m->flag = 0;
	m->guard = 0;
	queue_init(m->q);
}

void lock(lock_t *m) {
	while (TestAndSet(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	if (m->flag == 0) {
		m->flag = 1; // lock is acquired
		m->guard = 0;
	} else {
		queue_add(m->q, gettid());//将线程加入队列
        setpark();
		m->guard = 0;
		park();//调用线程休眠
	}
}

void unlock(lock_t *m) {
	while (TestAndSet(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	if (queue_empty(m->q))
		m->flag = 0; // let go of lock; no one wants it
	else
		unpark(queue_remove(m->q)); // hold lock (for next thread!)
	m->guard = 0;
}
```

##### 28.15 不同操作系统，不同实现

例如，Linux 提供了 futex，它类似于 Solaris 的接口，但提供了更多内核功能。具体来说， 每个 futex 都关联一个特定的物理内存位置，也有一个事先建好的内核队列。调用者通过 futex 调用（见下面的描述）来睡眠或者唤醒。

具体来说，有两个调用。调用 futex_wait(address, expected)时，如果 address 处的值等于 expected，就会让调线程睡眠。否则，调用立刻返回。调用 futex_wake(address)唤醒等待队 列中的一个线程。

```c
 void mutex_lock (int *mutex) { 
2 int v; 
3 /* Bit 31 was clear, we got the mutex (this is the fastpath) */ 
4 if (atomic_bit_test_set (mutex, 31) == 0) 
5 return; 
6 atomic_increment (mutex); 
7 while (1) { 
8 if (atomic_bit_test_set (mutex, 31) == 0) { 
9 atomic_decrement (mutex); 
10 return; 
11 } 
12 /* We have to wait now. First make sure the futex value 
13 we are monitoring is truly negative (i.e. locked). */ 
14 v = *mutex; 
15 if (v >= 0) 
16 continue; 
17 futex_wait (mutex, v); 
18 } 
19 } 
20 
21 void mutex_unlock (int *mutex) { 
22 /* Adding 0x80000000 to the counter results in 0 if and only if 
23 there are not other interested threads */ 
24 if (atomic_add_zero (mutex, 0x80000000)) 
25 return; 
26 
27 /* There are other threads waiting for this mutex, 
28 wake one of them up. */ 
29 futex_wake (mutex); 
```

##### 28.16 两阶段锁

Linux 采用的是一种古老的锁方案，多年来不断被采用，可以追溯到 20 世 纪 60 年代早期的 Dahm 锁。现在也称为两阶段锁（two-phase lock）。两阶段锁意识到 自旋可能很有用，尤其是在很快就要释放锁的场景。因此，两阶段锁的第一阶段会先自旋 一段时间，希望它可以获取锁

但是，如果第一个自旋阶段没有获得锁，第二阶段调用者会睡眠，直到锁可用。上文 的 Linux 锁就是这种锁，不过只自旋一次；更常见的方式是在循环中自旋固定的次数，然后 使用 futex 睡眠。

两阶段锁是又一个杂合（hybrid）方案的例子，即结合两种好想法得到更好的想法。当 然，硬件环境、线程数、其他负载等这些因素，都会影响锁的效果。事情总是这样，让单 个通用目标的锁，在所有可能的场景下都很好，这是巨大的挑战。